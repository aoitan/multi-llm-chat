# 新機能要件定義：コンテキスト圧縮機能

### 1. 概要

会話履歴が長くなることによる、APIのトークン数上限超過エラーの防止、APIコストの抑制、および応答速度の維持を目的として、LLMに送信するコンテキストのサイズを管理（圧縮）する機能を導入する。

### 2. 機能要件 (FR)

-   **FR-1: トークン上限の設定**
    -   送信するコンテキストの最大トークン数を、単一の設定値で管理できること。
    -   これにより、プロジェクトで使用するモデルの中で最もコンテキストウィンドウが小さいモデルに合わせて、上限値を調整できるようにする。

-   **FR-2: トークン数の計算**
    -   現在の会話履歴が、設定された最大トークン数を超えているかどうかを判定できること。

-   **FR-3: コンテキストの枝刈り（Pruning）**
    -   トークン数が上限を超える場合、設定された圧縮方式に従って、古い会話履歴を文脈から削除し、上限内に収まるように調整すること。

### 3. 圧縮方式の検討

圧縮方式には、それぞれ利点と欠点があります。

-   **方式A: スライディングウィンドウ**
    -   **方法**: 常に最新のN件の会話だけをコンテキストとして送信する。
    -   **長所**: 実装が非常にシンプル。直近の文脈は完全に保持される。
    -   **短所**: 会話の序盤で行った重要な指示や設定を忘れてしまう。

-   **方式B: 要約**
    -   **方法**: 古くなった会話履歴を、別のLLM（より安価で高速なモデル）を使って要約し、要約文で置き換える。
    -   **長所**: 会話全体の情報を（圧縮された形で）保持できる。
    -   **短所**: 要約処理自体にAPIコストと時間がかかる。要約の過程で重要なディテールが失われる可能性がある。

-   **方式C: ハイブリッド（要約＋スライディングウィンドウ）**
    -   **方法**: 会話の冒頭部分や重要な指示を「サマリー」として常に保持しつつ、直近の会話は「スライディングウィンドウ」でそのまま保持する。`コンテキスト = サマリー + 直近の会話` という構成。
    -   **長所**: 長期的な記憶と、直近の詳細な文脈の両方を維持できる、最もバランスの取れた方式。
    -   **短所**: 実装が最も複雑になる。

### 4. MVP設計提案

まずは、機能の恩恵を最も早く、かつシンプルに実現できる方法から着手することを提案します。

-   **設定方法**:
    -   `MLC_MAX_CONTEXT_TOKENS` という環境変数を導入し、最大トークン数を外部から設定可能にする。設定されていない場合は、安全なデフォルト値（例: 4096）にフォールバックする。

-   **トークン計算方法**:
    -   OpenAIが公開している高速なトークナイザライブラリ `tiktoken` を導入し、コンテキストのトークン数を概算する。

-   **圧縮方式**:
    -   MVPでは、最もシンプルで効果的な**方式A: スライディングウィンドウ**を採用する。
    -   実装としては、常にシステムプロンプト（もしあれば）と、最新のN件の会話履歴をAPIに送信するように履歴をフィルタリングする。

### 5. 将来の展望

-   MVPリリース後、ユーザーのフィードバックや利用状況に応じて、より高度な**方式C: ハイブリッド（要約＋スライディングウィンドウ）**の実装を検討する。
