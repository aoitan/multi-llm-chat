# 新機能要件定義：コンテキスト圧縮機能

### 1. 概要

会話履歴が長くなることによる、APIのトークン数上限超過エラーの防止、APIコストの抑制、および応答速度の維持を目的として、LLMに送信するコンテキストのサイズを管理（圧縮）する機能を導入する。

### 2. 機能要件 (FR)

-   **FR-1: 最大コンテキスト長の設定**
    -   送信するコンテキストの**最大コンテキスト長**をモデルごとに設定できること。
    -   環境変数（例: `GEMINI_MAX_CONTEXT_LENGTH`, `CHATGPT_MAX_CONTEXT_LENGTH`）で設定する。
    -   特定のモデルに対する設定がない場合は、汎用のデフォルト値（`DEFAULT_MAX_CONTEXT_LENGTH`）にフォールバックする。

-   **FR-2: トークン数の計算**
    -   現在の会話履歴のトークン数が、選択中のモデルに対応する**最大コンテキスト長**を超えているかどうかを判定できること。

-   **FR-3: コンテキストの枝刈り（Pruning）**
    -   トークン数が**最大コンテキスト長**を超える場合、設定された圧縮方式に従って、古い会話の**ターン**（ユーザー発言とAI応答のペア）を単位として文脈から削除し、上限内に収まるように調整すること。

-   **FR-4: システムプロンプトのトークン数チェック**
    -   システムプロンプト自体のトークン数が、選択中のモデルの**最大コンテキスト長**を超える場合、UI/CLI上でエラーを発生させてユーザーに通知し、送信を中止する。

-   **FR-5: 単一ターンの超過処理**
    -   システムプロンプトと最新の会話**ターン**だけで**最大コンテキスト長**を超える場合（例: 非常に長いテキストをペーストした場合など）、その旨をUI上でユーザーに通知し、APIへの送信を中止する。

### 3. 非機能要件 (NFR)

-   **NFR-1: トークナイザの拡張性**
    -   トークン数計算ロジックは、将来的にAnthropic/Claudeなど新しいモデルプロバイダの公式トークナイザを簡単に追加・差し替えできるよう、プロバイダ名をキーとしたディクショナリなどで管理し、プラガブルな設計とすること。

### 4. 圧縮方式の検討

圧縮方式には、それぞれ利点と欠点があります。実装の複雑さ、コスト、文脈保持能力のトレードオフを考慮して選択します。

-   **方式A: スライディングウィンドウ**
    -   **方法**: 常に最新の会話履歴を、設定された**最大コンテキスト長**の上限に収まるように保持する。
    -   **長所**: 実装が非常にシンプル。直近の文脈は完全に保持される。
    -   **短所**: 会話の序盤で行った重要な指示や設定を忘れてしまう可能性がある。

-   **方式B: 要約**
    -   **方法**: 古くなった会話履歴を、別のLLM（より安価で高速なモデル）を使って要約し、要約文で置き換える。
    -   **長所**: 会話全体の情報を（圧縮された形で）保持できる可能性がある。
    -   **短所**: 要約処理自体にAPIコストと時間がかかる。要約の過程で重要なディテールが失われる可能性がある。

-   **方式C: ハイブリッド（要約＋スライディングウィンドウ）**
    -   **方法**: 会話の冒頭部分や重要な指示を「サマリー」として常に保持しつつ、直近の会話は「スライディングウィンドウ」でそのまま保持する。`コンテキスト = サマリー + 直近の会話` という構成。
    -   **長所**: 長期的な記憶と、直近の詳細な文脈の両方を維持するバランスの取れたアプローチとなりうる。
    -   **短所**: 実装が最も複雑になり、要約のタイミングや内容を管理するロジックが必要。

### 5. MVP設計提案

まずは、機能の恩恵を最も早く、かつシンプルに実現できる方法から着手することを提案します。

-   **設定方法**:
    -   `CHATGPT_MAX_CONTEXT_LENGTH`、`GEMINI_MAX_CONTEXT_LENGTH` のような命名規則で、モデルごとの**最大コンテキスト長**を環境変数で設定可能にする。設定されていない場合は、安全なデフォルト値（例: 4096）にフォールバックする。

-   **トークン計算方法**:
    -   プロバイダごとにトークン計算関数を管理する仕組みを導入する。
    -   **OpenAI**: `tiktoken` ライブラリを使用する。
    -   **その他 (Gemini, Claude等)**: MVP段階では、暫定対応として文字数からトークン数を概算する。この際、日本語等のマルチバイト文字圏でトークン数が多くなる傾向を考慮し、安全側に倒した係数（例: `文字数 / 2`）を用いる。この概算が不正確であるリスクと、APIエラーが発生しうることはUI上でユーザーに通知する。

-   **圧縮方式**:
    -   MVPでは、最もシンプルで効果的な**方式A: スライディングウィンドウ**を採用する。
    -   実装としては、常にシステムプロンプト（もしあれば）を保持し、合計トークン数が**最大コンテキスト長**を超える場合は、上限内に収まるまで古い会話の**ターン**から順に削除する。

### 6. エラーハンドリング

-   **設定エラー**: 環境変数で設定された**最大コンテキスト長**が不正な値（数値以外など）の場合、アプリケーション起動時にエラーログを出力し、デフォルト値で動作する。
-   **APIエラー**: API呼び出し時に（圧縮処理を経てもなお）トークン数上限エラーが返却された場合、その旨をユーザーに通知し、コンテキストを手動で編集・短縮するよう促すメッセージを表示する。

### 7. 将来の展望

-   MVPリリース後、ユーザーのフィードバックや利用状況に応じて、より高度な**方式C: ハイブリッド（要約＋スライディングウィンドウ）**の実装を検討する。
-   各モデルプロバイダの公式トークナイザを順次導入し、トークン計算の精度を向上させる。
-   概算トークン数と、APIから返却される実際のトークン数（OpenAI APIなどが提供）の差を監視し、定期的にログ出力することで、概算精度の評価や係数チューニングに役立てる。
